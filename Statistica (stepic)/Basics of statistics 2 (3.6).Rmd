---
title: "Basics of statistics 2 (3.6)"
author: "Zelimkhan"
date: '2023-04-15'
output: html_document
---
# 3.1 Cluster analysis by k-means method
Я надеюсь, вы без труда поняли правильный ответ в прошлом тесте! Действительно, интересной особенностью кластерного анализа k - means является тот факт, что он включает в себя элемент случайности при выборе исходных позиций центроидов. В результате при многократном повторении кластеризации на одних и тех же данных мы можем получать различные варианты кластерного решения. Чем менее явно представлена в наших данных кластерзация наблюдений, тем более существенными могут оказаться различия. 

Давайте посмотрим на несколько вариантов кластеризации наших данных (результаты работы функции kmeans в R):
```{r}
library(ggplot2)
d <- iris[, c("Sepal.Length", "Petal.Width")]

fit <- kmeans(d, 3)
d$clusters <- factor(fit$cluster)

ggplot(d, aes(Sepal.Length, Petal.Width, col = clusters))+
    geom_point(size = 2)+
    theme_bw() 
```



# 3.4 Hierarhical cluster analysis

Особое внимание стоит уделить тому, как происходит выбор числа кластеров при иерархической кластеризации. Как я сказал, в этом случае у нас есть некоторая свобода в том, какое число кластеров выбрать, но то как исходные данные разделятся на выбранное число кластеров все-таки подчиняется логике работы алгоритма. Рассмотрим пример кластеризации следующего набора данных:
```{r}
library(ggplot2) 
library(ggrepel) # для симпатичной подписи точек на графике

x <- rnorm(10)
y <- rnorm(10)
test_data <- data.frame(x, y)
test_data$labels <- 1:10

ggplot(test_data, aes(x, y, label = labels)) +
    geom_point() +
    geom_text_repel()

d = dist(test_data)
fit <- hclust(d, method = "single")
plot(fit, labels = test_data$labels)
rect.hclust(fit, 2) # укажите желаемое число кластеров, сейчас стоит 2
```

Перед вами пример иерархической кластеризации: филогенетическое дерево 20 образцов, построенное по отличиям в их ДНК. Если вы не знаете, что такое ДНК и филогенетическое дерево, читайте дальше, это совсем не помешает решить задание! Если знаете, то почувствуйте себя в родной стихии.
```{r}
library(ape)
set.seed(222)
tr <- rtree(20, tip.label = c("B","C","D","E","F","G","H","I","J","K","L","M","N","O","P","Q","R","S","T","U")) 
#левое дерево
plot.phylo(tr) 
#правое дерево 
plot.phylo(tr, use.edge.length = FALSE)
```



# Step 1 of 6
Напишите функцию smart_hclust, которая получает на вход dataframe  с произвольным числом количественных переменных и число кластеров, которое необходимо выделить при помощи иерархической кластеризации.
Функция должна в исходный набор данных добавлять новую переменную фактор - cluster  -- номер кластера, к которому отнесено каждое из наблюдений.
Пример работы функции:

```{r}
test_data <- read.csv("https://stepic.org/media/attachments/course/524/test_data_hclust.csv")
str(test_data)
smart_hclust(test_data, 3)
```
   X1 X2 X3 X4 X5    cluster
1  11  7 10 10  8       1
2   9 10 10  8  6       1
3   9  2 12 14 11       1
4   9 11  8 10  3       1
5   7  9 10 11 14       1
6   9 11  9  6  9       1
7  16 20 22 19 16       2
8  23 18 21 24 16       2
9  15 21 14 21 21       3
10 19 20 15 17 17       3
11 20 24 21 20 19       2
12 22 19 27 22 19       2

В этой и следующей задаче на кластерный анализ предполагается, что мы используем функцию hclust() для 
кластеризации данных с параметрами по умолчанию:
```{r}
hclust(d, method = "complete", members = NULL)
```
Для расчета матрицы расстояний предполагается, что используется функция dist() также с параметрами по умолчанию:
```{r}
dist(x, method = "euclidean", diag = FALSE, upper = FALSE, p = 2)
```
Для выделения желаемого числа кластеров по результатам иерархической кластеризации воспользуйтесь функцией cutree().
Иными словами, для кластеризации данных swiss на три кластера мы бы использовали команды:
```{r}
dist_matrix <- dist(swiss) # расчет матрицы расстояний
fit <- hclust(dist_matrix) # иерархическая кластеризация 
cluster <- cutree(fit, 3) # номер кластера для каждого наблюдения
```

```{r}
smart_hclust<-  function(test_data, cluster_number){


}
```


# Step 2 of 6
Интересной особенностью кластерного анализа является тот факт, что мы получаем только итоговый ответ, к какому кластеру принадлежит каждое наблюдение. Однако мы не знаем, по каким переменным различаются выделенные кластеры. Поэтому, если нас интересует не только сам факт того, что мы смогли выделить кластеры в наших данных, но мы также хотим понять, чем же они различаются, разумно сравнить кластеры между собой по имеющимся переменным.

Напишите функцию get_difference, которая получает на вход два аргумента: 

test_data — набор данных с произвольным числом количественных переменных.
n_cluster — число кластеров, которое нужно выделить в данных при помощи иерархической кластеризации.
Функция должна вернуть названия переменных, по которым были обнаружен значимые различия между выделенными кластерами (p < 0.05)﻿. Иными словами, после того, как мы выделили заданное число кластеров, мы добавляем в исходные данные новую группирующую переменную — номер кластера, и сравниваем получившиеся группы между собой по количественным переменным при помощи дисперсионного анализа.
Пример работы функции:
В первом наборе данных, очевидно, что два кластера будут значимо различаться только по переменной V2.
      рис.1
```{r}
test_data <- read.csv("https://stepic.org/media/attachments/course/524/cluster_1.csv")
get_difference(test_data, 2)
```
Во втором наборе данных при выделении двух кластеров значимые различия получаются по обеим переменным.
```{r}
test_data <- read.csv("https://stepic.org/media/attachments/course/524/cluster_2.csv")
get_difference(test_data, 2)
```
Подсказки:
Не забудьте перевести переменную с номером кластера в фактор! 
Вы можете использовать вашу функцию из предыдущего задания.
Для поиска различий используйте ANOVA (функция aov). Давайте договоримся, что для наших целей мы не будем проверять данные на соответствие требованиями к применению этого критерия и не будем думать о поправке на множественные сравнения.

```{r}
get_difference<-  function(test_data, n_cluster){

}
```

# Step 3 of 6
Напишите функцию get_pc, которая получает на вход dataframe с произвольным числом количественных переменных. Функция должна выполнять анализ главных компонент и добавлять в исходные данные две новые колонки со значениями первой и второй главной компоненты. Новые переменные должны называться "PC1"  и "PC2" соответственно.
Пример работы функции:
```{r}
test_data <- read.csv("https://stepic.org/media/attachments/course/524/pca_test.csv")
test_data
get_pc(test_data)
```
Для выполнения анализа главных компонент используйте функцию prcomp(). Изучите результат применения этой функции к данным, чтобы найти, где хранятся значения выделенных главных компонент.

```{r}
get_pc <- function(d){

}
```

# Step 4 of 6
Усложним предыдущую задачу! Напишите функцию get_pca2, которая принимает на вход dataframe с произвольным числом количественных переменных. Функция должна рассчитать, какое минимальное число главных компонент объясняет больше 90% изменчивости в исходных данных и добавлять значения этих компонент в исходный dataframe в виде новых переменных.
Рассмотрим работу функции на примере встроенных данных swiss:
```{r}
fit <- prcomp(swiss)
summary(fit)
```
Importance of components:
                          PC1     PC2      PC3     PC4     PC5    PC6
Standard deviation     43.836 21.6022 12.05342 4.75916 3.65754 2.4882
Proportion of Variance  0.746  0.1812  0.05641 0.00879 0.00519 0.0024
Cumulative Proportion   0.746  0.9272  0.98361 0.99240 0.99760 1.0000

в нашем случае уже две компоненты объясняют больше 90% изменчивости
значит значения первых двух компонент нужно добавить в исходные данные

пример работы функции:
```{r}
result  <- get_pca2(swiss)
str(result)
```
'data.frame':  47 obs. of  8 variables:
 $ Fertility       : num  80.2 83.1 92.5 85.8 76.9 76.1 83.8 92.4 82.4 82.9 ...
 $ Agriculture     : num  17 45.1 39.7 36.5 43.5 35.3 70.2 67.8 53.3 45.2 ...
 $ Examination     : int  15 6 5 12 17 9 16 14 12 16 ...
 $ Education       : int  12 9 5 7 15 7 7 8 7 13 ...
 $ Catholic        : num  9.96 84.84 93.4 33.77 5.16 ...
 $ Infant.Mortality: num  22.2 22.2 20.2 20.3 20.6 26.6 23.6 24.9 21 24.4 ...
 $ PC1             : num  37.03 -42.8 -51.08 7.72 35.03 ...
 $ PC2             : num  -17.43 -14.69 -19.27 -5.46 5.13 ...
 
```{r}
get_pca2 <- function(data){

}
```

# Step 5 of 6
Задача для Чака Норриса.

Как я говорил, метод главных компонент может применяться для борьбы с мультиколлинеарностью в данных (ситуация, когда некоторые переменные очень сильно коррелируют между собой). Однако иногда некоторые переменные не просто сильно взаимосвязаны, но могут представлять линейную комбинацию друг друга. На такие переменные лучше сразу взглянуть повнимательнее и выяснить, откуда они взялись в наших данных.

Напишите функцию is_multicol, которая получает на вход dataframe произвольного размера с количественными переменными. Функция должна проверять существование строгой мультиколлинеарности, а именно наличие линейной комбинации между предикторами. Линейной комбинацией является ситуация, когда одна переменная может быть выражена через другую переменную при помощи уравнения V1 = K * V2 + b
Например, V1 = V2 + 4 или V1 = V2 - 5
Функция возвращает имена переменных, между которыми есть линейная зависимость или cобщение "There is no collinearity in the data".
```{r}
#В данных нет мультиколлинеарности
test_data <- read.csv("https://stepic.org/media/attachments/course/524/Norris_1.csv")
```
 V1 V2 V3 V4
1 22 20 18 20
2 16 28 31 15
3 14 24  7 16

```{r}
is_multicol(test_data)
```
[1] "There is no collinearity in the data"

V1 = V2 + 1
```{r}
test_data <- read.csv("https://stepic.org/media/attachments/course/524/Norris_2.csv")
```
V1 V2 V3 V4
1 13 12  7 11
2 15 14 13 10
3  8  7 11 16
```{r}
is_multicol(test_data)
```
[1] "V2" "V1"

V1 = V2 + 1 и V3 = V4 - 2
```{r}
test_data <- read.csv("https://stepic.org/media/attachments/course/524/Norris_3.csv")
```
V1 V2 V3 V4
1 20 19 12 14
2 11 10  5  7
3 12 11 10 12
```{r}
is_multicol(test_data)
```
[1] "V2" "V1" "V4" "V3"

```{r}
is_multicol <- function(d){

}
```


# Step 6 of 6
Вот и подходит к концу наш курс. Давайте построим финальный график!
В данных swiss, используя все переменные, выделите два кластера при помощи иерархической кластеризации и сохраните значение кластеров как фактор в переменную cluster. Должно получиться:
```{r}
str(swiss)
```
'data.frame':  47 obs. of  7 variables:
 $ Fertility       : num  80.2 83.1 92.5 85.8 76.9 76.1 83.8 92.4 82.4 82.9 ...
 $ Agriculture     : num  17 45.1 39.7 36.5 43.5 35.3 70.2 67.8 53.3 45.2 ...
 $ Examination     : int  15 6 5 12 17 9 16 14 12 16 ...
 $ Education       : int  12 9 5 7 15 7 7 8 7 13 ...
 $ Catholic        : num  9.96 84.84 93.4 33.77 5.16 ...
 $ Infant.Mortality: num  22.2 22.2 20.2 20.3 20.6 26.6 23.6 24.9 21 24.4 ...
 $ cluster         : Factor w/ 2 levels "1","2": 1 2 2 1 1 2 2 2 2 2 ...
 
Затем визуализируйте взаимосвязь переменных  Education и  Catholic в двух выделенных кластерах, чтобы получился следующий график:
            Рис. 1
Подсказки:
— для визуализации линейной регрессии пользуйтесь geom_smooth();
— точки проще всего добавить с помощью geom_point().
```{r}
# сначала создайте переменную cluster в данных swiss

# дополните код, чтобы получить график
library(ggplot2)
my_plot <- ggplot(swiss, aes(Education, Catholic, col = ))
```



































